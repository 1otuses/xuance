{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# New Algorithm\n",
    "\n",
    "We allow users create their own customized algorithm outside of the default in XuanCe.\n",
    "\n",
    "This tutorial walks you through the process of creating, training,\n",
    "and testing a custom off-policy reinforcement learning (RL) agent using the XuanCe framework.\n",
    "The demo involves defining a custom policy, learner, and agent while using XuanCe’s modular architecture for RL experiments.\n",
    "\n",
    "## Step 1: Define the Policy Module\n",
    "\n",
    "The policy is the brain of the agent.\n",
    "It maps observations to actions, optionally through a value function. Here, we define a custom policy MyPolicy:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6617823f8184e7fb"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class MyPolicy(nn.Module):\n",
    "    \"\"\"\n",
    "    An example of self-defined policy.\n",
    "\n",
    "    Args:\n",
    "        representation (nn.Module): A neural network module responsible for extracting meaningful features from the raw observations provided by the environment.\n",
    "        hidden_dim (int): Specifies the number of units in each hidden layer, determining the model’s capacity to capture complex patterns.\n",
    "        n_actions (int): The total number of discrete actions available to the agent in the environment.\n",
    "        device (torch.device): The calculating device.\n",
    "\n",
    "\n",
    "    Note: The inputs to the __init__ method are not rigidly defined. You can extend or modify them as needed to accommodate additional settings or configurations specific to your application.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, representation: nn.Module, hidden_dim: int, n_actions: int, device: torch.device):\n",
    "        super(MyPolicy, self).__init__()\n",
    "        self.representation = representation  # Specify the representation.\n",
    "        self.feature_dim = self.representation.output_shapes['state'][0]  # Dimension of the representation's output.\n",
    "        self.q_net = nn.Sequential(\n",
    "            nn.Linear(self.feature_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, n_actions),\n",
    "        ).to(device)  # The Q network.\n",
    "        self.target_q_net = deepcopy(self.q_net)  # Target Q network.\n",
    "\n",
    "    def forward(self, observation):\n",
    "        output_rep = self.representation(observation)  # Get the output of the representation module.\n",
    "        output = self.q_net(output_rep['state'])  # Get the output of the Q network.\n",
    "        argmax_action = output.argmax(dim=-1)  # Get greedy actions.\n",
    "        return output_rep, argmax_action, output\n",
    "\n",
    "    def target(self, observation):\n",
    "        outputs_target = self.representation(observation)  # Get the output of the representation module.\n",
    "        Q_target = self.target_q_net(outputs_target['state'])  # Get the output of the target Q network.\n",
    "        argmax_action = Q_target.argmax(dim=-1)  # Get greedy actions that output by target Q network.\n",
    "        return outputs_target, argmax_action.detach(), Q_target.detach()\n",
    "\n",
    "    def copy_target(self):  # Reset the parameters of target Q network as the Q network.\n",
    "        for ep, tp in zip(self.q_net.parameters(), self.target_q_net.parameters()):\n",
    "            tp.data.copy_(ep)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "42e262f4939423a4"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Key Points:\n",
    "\n",
    "- representation module: Extracts state features, decoupling feature engineering from Q-value computation.\n",
    "- networks: The policy uses a feedforward neural network to calculate actions and estimate Q-values.\n",
    "- device: The device choice should align with that of the other modules.\n",
    "\n",
    "## Step 2: Define the Learner\n",
    "\n",
    "The learner manages the policy optimization process,\n",
    "including computing loss, performing gradient updates, and synchronizing target networks."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "59aa48f28a8e9a4c"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class MyLearner(Learner):\n",
    "    def __init__(self, config, policy):\n",
    "        super(MyLearner, self).__init__(config, policy)\n",
    "        # Build the optimizer.\n",
    "        self.optimizer = torch.optim.Adam(self.policy.parameters(), self.config.learning_rate, eps=1e-5)\n",
    "        self.loss = nn.MSELoss()  # Build a loss function.\n",
    "        self.sync_frequency = config.sync_frequency  # The period to synchronize the target network.\n",
    "\n",
    "    def update(self, **samples):\n",
    "        info = {}\n",
    "        self.iterations += 1\n",
    "        '''Get a batch of training samples.'''\n",
    "        obs_batch = torch.as_tensor(samples['obs'], device=self.device)\n",
    "        act_batch = torch.as_tensor(samples['actions'], device=self.device)\n",
    "        next_batch = torch.as_tensor(samples['obs_next'], device=self.device)\n",
    "        rew_batch = torch.as_tensor(samples['rewards'], device=self.device)\n",
    "        ter_batch = torch.as_tensor(samples['terminals'], dtype=torch.float, device=self.device)\n",
    "\n",
    "        # Feedforward steps.\n",
    "        _, _, q_eval = self.policy(obs_batch)\n",
    "        _, _, q_next = self.policy.target(next_batch)\n",
    "        q_next_action = q_next.max(dim=-1).values\n",
    "        q_eval_action = q_eval.gather(-1, act_batch.long().unsqueeze(-1)).reshape(-1)\n",
    "        target_value = rew_batch + (1 - ter_batch) * self.gamma * q_next_action\n",
    "        loss = self.loss(q_eval_action, target_value.detach())\n",
    "\n",
    "        # Backward and optimizing steps.\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # Synchronize the target network\n",
    "        if self.iterations % self.sync_frequency == 0:\n",
    "            self.policy.copy_target()\n",
    "\n",
    "        # Set the variables you need to observe.\n",
    "        info.update({'loss': loss.item(),\n",
    "                     'iterations': self.iterations,\n",
    "                     'q_eval_action': q_eval_action.mean().item()})\n",
    "\n",
    "        return info"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d4ac2eba7c1a26ee"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Key Points:\n",
    "\n",
    "- optimizer: The pytorch's optimizer should be selected in the __init__ method.\n",
    "- update: In this method, we can get a batch of samples and use them to calculate loss values and back propagation.\n",
    "- info: The users can add arbitrarily .\n",
    "\n",
    "## Step 3: Define the Agent\n",
    "\n",
    "The agent combines the policy, learner, and environment interaction to create a complete RL pipeline."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f858efa9cceeec08"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class MyAgent(OffPolicyAgent):\n",
    "    def __init__(self, config, envs):\n",
    "        super(MyAgent, self).__init__(config, envs)\n",
    "        self.policy = self._build_policy()  # Build the policy module.\n",
    "        self.memory = self._build_memory()  # Build the replay buffer.\n",
    "        REGISTRY_Learners['MyLearner'] = MyLearner  # Registry your pre-defined learner.\n",
    "        self.learner = self._build_learner(self.config, self.policy)  # Build the learner.\n",
    "\n",
    "    def _build_policy(self):\n",
    "        # First create the representation module.\n",
    "        representation = self._build_representation(\"Basic_MLP\", self.observation_space, self.config)\n",
    "        # Build your customized policy module.\n",
    "        policy = MyPolicy(representation, 64, self.action_space.n, self.config.device)\n",
    "        return policy"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "376c761788be3d39"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Key Points:\n",
    "\n",
    "- Policy: Build the custom policy and learner defined earlier.\n",
    "- Memory: Build experience replay to break correlations in training data.\n",
    "- Learner: Register MyLearner for easy configuration.\n",
    "\n",
    "## Step 4: Build and Run Your Agent.\n",
    "\n",
    "Finally, we can create the agent and make environments to train the model."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ea308b2165fff43a"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "config = get_configs(file_dir=\"./new_rl.yaml\")  # Get the config settings from .yaml file.\n",
    "config = Namespace(**config)  # Convert the config from dict to argparse.\n",
    "envs = make_envs(config)  # Make vectorized environments.\n",
    "agent = MyAgent(config, envs)  # Instantiate your pre-build agent class.\n",
    "\n",
    "if not config.test_mode:  # Training mode.\n",
    "    agent.train(config.running_steps // envs.num_envs)  # Train your agent.\n",
    "    agent.save_model(\"final_train_model.pth\")  # After training, save the model.\n",
    "else:  # Testing mode.\n",
    "    config.parallels = 1  # Test on one environment.\n",
    "    env_fn = lambda: make_envs(config)  # The method to create testing environment.\n",
    "    agent.load_model(agent.model_dir_load)  # Load pre-trained model.\n",
    "    scores = agent.test(env_fn, config.test_episode)  # Test your agent.\n",
    "\n",
    "agent.finish()  # Finish the agent.\n",
    "envs.close()  # Close the environments."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4047c2c21aabd48b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "The source code of this example can be visited at the following link:\n",
    "\n",
    "[https://github.com/agi-brain/xuance/blob/master/examples/new_algorithm/new_rl.py](https://github.com/agi-brain/xuance/blob/master/examples/new_algorithm/new_rl.py)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c0d0b3fc1befe66"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
